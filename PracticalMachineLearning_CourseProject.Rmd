---
title: "Practical Machine Learning Course Project"
author: "Dana Lipperman"
date: "May 13, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. Data from accelerometers on the belt, forearm, arm and dumbell of 6 participants will be used in this project.  Participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

The goal of this project is to predict the manner in which the participants did the exercise.  This report will describe how I built the model, how I used cross validation, what I think the expected out of sample error is, and why I made the choices I did.

## Exploratory Data Analysis

```{r}
# Import training and test datasets
trainingUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testingUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainingUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testingUrl), na.strings=c("NA","#DIV/0!",""))

# Check dimensions of training and test datasets
dim(training)
dim(testing)
```

The training data contains 19,622 observations and 160 variables and the testing dataset contains 20 observations and 160 variables as well.  The "class" variable in the training dataset (the last column) is the outcome to predict.

## Data Cleaning

The next step is to clean the dataset to get rid of missing observations and variables that will not contribute to our analysis in any way, which will help with run time when performing a random forest later on in this report.

```{r}
# The following line of code tells us that there isn't a single observation in the training dataset with no missing values in all columns
sum(complete.cases(training))

# Remove columns that contain NAs (we could have chosen instead to use some kind of threshold of % of NAs allowed in a column but decided to remove them entirely and see how well the model predicted the outcome without all NAs)
training_noNA <- training[ , colSums(is.na(training)) == 0]
testing_noNA <- testing[ , colSums(is.na(testing)) == 0]

# Remove columns that do not contribute to our analysis any way, the row count variable and raw time stamps
training_clean <- training_noNA[ , -c(1,3,4,5)]
testing_clean <- testing_noNA[ , -c(1,3,4,5,60)]

# Check that all observations are complete
sum(!complete.cases(training_clean))
```

## Split data into training and validation datasets

Next, I split the cleaned training dataset into a pure training dataset (75%) and a validation dataset (25%) to test our prediction model. I will use the validation dataset to conduct cross validation later on. This cross validation will allow us to try out the model on a smaller validation dataset with the prediction variable before using it on the testing dataset.

```{r}
library(caret)
library(randomForest)
library(rpart)

set.seed(1234)
inTrain <- createDataPartition(training_clean$classe, p=0.75, list=F)
training_2 <- training_clean[inTrain, ]
validation <- training_clean[-inTrain, ]
```

# Prediction Model - Decision Trees

First I'll try fitting a prediction model using decision trees.

```{r}
set.seed(123)
Dt <- rpart(classe ~ ., data = training_2, method = "class")
Dt_predict <- predict(Dt, validation, type = "class")
Dt_model <- confusionMatrix(Dt_predict, validation$classe)
Dt_model
plot(Dt_model$table, col = Dt_model$byClass, main = "Decision Tree Confusion Matrix")
```

The estimated accuracy of this model is 73.27% and the estimated out of sample error is 26.73%.  This isn't a great fit so next I'll try a second prediction model, the Random Forest.

# Prediction Model - Random Forest

Next I will fit a prediction model using a Random Forest algorithm because it automatically picks important variables and can be used for both classification and regression tasks.  In addition, Random Forest is easy to use and often produces a good prediction result. I used a 5-fold cross validation when using the Random Forest algorithm.  

```{r}
Rf_control <- trainControl(method = "cv", number = 5, verboseIter = FALSE)
Rf_model <- train(classe ~ ., data = training_2, method = "rf", trControl = Rf_control, ntree = 250, na.action = na.pass)
Rf_model
```

Next I estimated the performance of the Random Forest model on the validation dataset.

```{r}
Rf_predict <- predict(Rf_model, validation)
Cm <- confusionMatrix(validation$classe, Rf_predict)
plot(Cm$table, col = Cm$byClass, main = "Random Forest Confusion Matrix")

accuracy <- postResample(Rf_predict, validation$classe)
accuracy
sampleerror <- 1 - as.numeric(Cm$overall[1])
sampleerror
```

The estimated accuracy of this model is 99.94% and the estimated out of sample error is 0.06%. The random forest algorithm provides a better predictor of classe than using decisions trees and therefore I will use this model to predict the classes in the testing dataset for the quiz.

## Predicting Results for the Testing dataset

For the purposes of the quiz, I will use the random forest prediction model above to predict the classe for the 20 observations.

```{r}
testing_predict <- predict(Rf_model, testing_clean)
testing_predict
```
